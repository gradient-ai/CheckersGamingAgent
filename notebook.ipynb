{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Get checkers.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget \"https://raw.githubusercontent.com/adilmrk/checkers/main/checkers.py\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create get metrics function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras import regularizers\n",
    "import checkers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "def get_metrics(board):  # returns [label, 10 labeling metrics]\n",
    "\tb = expand(board)\n",
    "\n",
    "\tcapped = num_captured(b)\n",
    "\tpotential = possible_moves(b) - possible_moves(reverse(b))\n",
    "\tmen = num_men(b) - num_men(-b)\n",
    "\tkings = num_kings(b) - num_kings(-b)\n",
    "\tcaps = capturables(b) - capturables(reverse(b))\n",
    "\tsemicaps = semicapturables(b)\n",
    "\tuncaps = uncapturables(b) - uncapturables(reverse(b))\n",
    "\tmid = at_middle(b) - at_middle(-b)\n",
    "\tfar = at_enemy(b) - at_enemy(reverse(b))\n",
    "\twon = game_winner(b)\n",
    "\n",
    "\tscore = 4*capped + potential + men + 3*kings + \\\n",
    "\t\tcaps + 2*semicaps + 3*uncaps + 2*mid + 3*far + 100*won\n",
    "\tif (score < 0):\n",
    "\t\treturn np.array([-1, capped, potential, men, kings, caps, semicaps, uncaps, mid, far, won])\n",
    "\telse:\n",
    "\t\treturn np.array([1, capped, potential, men, kings, caps, semicaps, uncaps, mid, far, won])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create metrics_model & model heuristic metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Metrics model, which only looks at heuristic scoring metrics used for labeling\n",
    "metrics_model = Sequential()\n",
    "metrics_model.add(Dense(32, activation='relu', input_dim=10))\n",
    "metrics_model.add(Dense(16, activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "# output is passed to relu() because labels are binary\n",
    "metrics_model.add(\n",
    "\tDense(1, activation='relu',  kernel_regularizer=regularizers.l2(0.1)))\n",
    "metrics_model.compile(\n",
    "\toptimizer='nadam', loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "\n",
    "start_board = checkers.expand(checkers.np_board())\n",
    "boards_list = checkers.generate_next(start_board)\n",
    "branching_position = 0\n",
    "nmbr_generated_game = 10000\n",
    "while len(boards_list) < nmbr_generated_game:\n",
    "\ttemp = len(boards_list) - 1\n",
    "\tfor i in range(branching_position, len(boards_list)):\n",
    "\t\tif (checkers.possible_moves(checkers.reverse(checkers.expand(boards_list[i]))) > 0):\n",
    "\t\t\tboards_list = np.vstack((boards_list, checkers.generate_next(\n",
    "\t\t\t\tcheckers.reverse(checkers.expand(boards_list[i])))))\n",
    "\tbranching_position = temp\n",
    "\n",
    "# calculate/save heuristic metrics for each game state\n",
    "metrics = np.zeros((0, 10))\n",
    "winning = np.zeros((0, 1))\n",
    "\n",
    "for board in boards_list[:nmbr_generated_game]:\n",
    "\ttemp = checkers.get_metrics(board)\n",
    "\tmetrics = np.vstack((metrics, temp[1:]))\n",
    "\twinning = np.vstack((winning, temp[0]))\n",
    "\n",
    "# fit the metrics model\n",
    "history = metrics_model.fit(\n",
    "\tmetrics, winning, epochs=32, batch_size=64, verbose=0)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# History for accuracy\n",
    "plot.plot(history.history['acc'])\n",
    "plot.plot(history.history['val_acc'])\n",
    "plot.title('model accuracy')\n",
    "plot.ylabel('accuracy')\n",
    "plot.xlabel('epoch')\n",
    "plot.legend(['train', 'validation'], loc='upper left')\n",
    "plot.show()\n",
    "\n",
    "# History for loss\n",
    "plot.plot(history.history['loss'])\n",
    "plot.plot(history.history['val_loss'])\n",
    "plot.title('model loss')\n",
    "plot.ylabel('loss')\n",
    "plot.xlabel('epoch')\n",
    "plot.legend(['train', 'validation'], loc='upper left')\n",
    "plot.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instnatiate board model and heuristic metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Board model\n",
    "board_model = Sequential()\n",
    "\n",
    "# input dimensions is 32 board position values\n",
    "board_model.add(Dense(64, activation='relu', input_dim=32))\n",
    "\n",
    "# use regularizers, to prevent fitting noisy labels\n",
    "board_model.add(Dense(32, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "board_model.add(Dense(16, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # 16\n",
    "board_model.add(Dense(8, activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))  # 8\n",
    "\n",
    "# output isn't squashed, because it might lose information\n",
    "board_model.add(Dense(1, activation='linear',\n",
    "                kernel_regularizer=regularizers.l2(0.01)))\n",
    "board_model.compile(optimizer='nadam', loss='binary_crossentropy')\n",
    "\n",
    "# calculate heuristic metric for data\n",
    "metrics = np.zeros((0, 10))\n",
    "winning = np.zeros((0, 1))\n",
    "data = boards_list\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the model and save weights"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for board in data:\n",
    "\ttemp = checkers.get_metrics(board)\n",
    "\tmetrics = np.vstack((metrics, temp[1:]))\n",
    "\twinning = np.zeros((0, 1))\n",
    "\n",
    "# calculate probilistic (noisy) labels\n",
    "probabilistic = metrics_model.predict_on_batch(metrics)\n",
    "\n",
    "# fit labels to {-1, 1}\n",
    "probabilistic = np.sign(probabilistic)\n",
    "\n",
    "# calculate confidence score for each probabilistic label using error between probabilistic and weak label\n",
    "confidence = 1/(1 + np.absolute(winning - probabilistic[:, 0]))\n",
    "\n",
    "# pass to the Board model\n",
    "board_model.fit(data, probabilistic, epochs=32, batch_size=64,\n",
    "                sample_weight=confidence, verbose=0)\n",
    "\n",
    "board_json = board_model.to_json()\n",
    "with open('board_model.json', 'w') as json_file:\n",
    "\tjson_file.write(board_json)\n",
    "board_model.save_weights('board_model.h5')\n",
    "\n",
    "print('Checkers Board Model saved to: board_model.json/h5')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcing the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "json_file = open('board_model.json', 'r')\n",
    "board_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "reinforced_model = model_from_json(board_json)\n",
    "reinforced_model.load_weights('board_model.h5')\n",
    "reinforced_model.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "data = np.zeros((1, 32))\n",
    "labels = np.zeros(1)\n",
    "win = lose = draw = 0\n",
    "winrates = []\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.95\n",
    "\n",
    "for gen in range(0, 50):\n",
    "\tfor game in range(0, 200):\n",
    "\t\ttemp_data = np.zeros((1, 32))\n",
    "\t\tboard = checkers.expand(checkers.np_board())\n",
    "\t\tplayer = np.sign(np.random.random() - 0.5)\n",
    "\t\tturn = 0\n",
    "\t\twhile (True):\n",
    "\t\t\tmoved = False\n",
    "\t\t\tboards = np.zeros((0, 32))\n",
    "\t\t\tif (player == 1):\n",
    "\t\t\t\tboards = checkers.generate_next(board)\n",
    "\t\t\telse:\n",
    "\t\t\t\tboards = checkers.generate_next(checkers.reverse(board))\n",
    "\n",
    "\t\t\tscores = reinforced_model.predict_on_batch(boards)\n",
    "\t\t\tmax_index = np.argmax(scores)\n",
    "\t\t\tbest = boards[max_index]\n",
    "\n",
    "\t\t\tif (player == 1):\n",
    "\t\t\t\tboard = checkers.expand(best)\n",
    "\t\t\t\ttemp_data = np.vstack((temp_data, checkers.compress(board)))\n",
    "\t\t\telse:\n",
    "\t\t\t\tboard = checkers.reverse(checkers.expand(best))\n",
    "\n",
    "\t\t\tplayer = -player\n",
    "\n",
    "\t\t\t# punish losing games, reward winners  & drawish games reaching more than 200 turns\n",
    "\t\t\twinner = checkers.game_winner(board)\n",
    "\t\t\tif (winner == 1 or (winner == 0 and turn >= 200)):\n",
    "\t\t\t\tif winner == 1:\n",
    "\t\t\t\t\twin = win + 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdraw = draw + 1\n",
    "\t\t\t\treward = 10\n",
    "\t\t\t\told_prediction = reinforced_model.predict_on_batch(temp_data[1:])\n",
    "\t\t\t\toptimal_futur_value = np.ones(old_prediction.shape)\n",
    "\t\t\t\ttemp_labels = old_prediction + learning_rate * \\\n",
    "\t\t\t\t\t(reward + discount_factor * optimal_futur_value - old_prediction)\n",
    "\t\t\t\tdata = np.vstack((data, temp_data[1:]))\n",
    "\t\t\t\tlabels = np.vstack((labels, temp_labels))\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif (winner == -1):\n",
    "\t\t\t\tlose = lose + 1\n",
    "\t\t\t\treward = -10\n",
    "\t\t\t\told_prediction = reinforced_model.predict_on_batch(temp_data[1:])\n",
    "\t\t\t\toptimal_futur_value = -1*np.ones(old_prediction.shape)\n",
    "\t\t\t\ttemp_labels = old_prediction + learning_rate * \\\n",
    "\t\t\t\t\t(reward + discount_factor * optimal_futur_value - old_prediction)\n",
    "\t\t\t\tdata = np.vstack((data, temp_data[1:]))\n",
    "\t\t\t\tlabels = np.vstack((labels, temp_labels))\n",
    "\t\t\t\tbreak\n",
    "\t\t\tturn = turn + 1\n",
    "\n",
    "\t\tif ((game+1) % 200 == 0):\n",
    "\t\t\treinforced_model.fit(data[1:], labels[1:],\n",
    "\t\t\t                     epochs=16, batch_size=256, verbose=0)\n",
    "\t\t\tdata = np.zeros((1, 32))\n",
    "\t\t\tlabels = np.zeros(1)\n",
    "\twinrate = int((win+draw)/(win+draw+lose)*100)\n",
    "\twinrates.append(winrate)\n",
    "\n",
    "\treinforced_model.save_weights('reinforced_model.h5')\n",
    "\n",
    "print('Checkers Board Model updated by reinforcement learning & saved to: reinforced_model.json/h5')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generations = range(0, 500)\n",
    "print(\"Final win/draw rate : \" + str(winrates[499])+\"%\")\n",
    "plot.plot(generations, winrates)\n",
    "plot.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def best_move(board):\n",
    "  compressed_board = checkers.compress(board)\n",
    "  boards = np.zeros((0, 32))\n",
    "  boards = checkers.generate_next(board)\n",
    "  scores = reinforced_model.predict_on_batch(boards)\n",
    "  max_index = np.argmax(scores)\n",
    "  best = boards[max_index]\n",
    "  return best\n",
    "\n",
    "\n",
    "def print_board(board):\n",
    "  for row in board:\n",
    "    for square in row:\n",
    "      if square == 1:\n",
    "        caracter = \"|O\"\n",
    "      elif square == -1:\n",
    "        caracter = \"|X\"\n",
    "      else:\n",
    "        caracter = \"| \"\n",
    "      print(str(caracter), end='')\n",
    "    print('|')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start_board = [1, 1, 1, 1,  1, 1, 1, 0,  1, 0, 0, 1,  0, 0, 1, 0,\n",
    "               0, 0, 0, 0,  0, 0, -1, -1,  -1, -1, -1, -1,  -1, -1, -1, -1]\n",
    "start_board = checkers.expand(start_board)\n",
    "next_board = checkers.expand(best_move(start_board))\n",
    "\n",
    "print(\"Starting position : \")\n",
    "print_board(start_board)\n",
    "\n",
    "print(\"\\nBest next move : \")\n",
    "print_board(next_board)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}